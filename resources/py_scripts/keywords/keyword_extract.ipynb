{
 "metadata": {
  "name": "",
  "signature": "sha256:7dd15b91321e67759e47c399a0e3a7b6f192b04b966bb39392b5abf0b99c8a3a"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.corpus import gutenberg\n",
      "import random\n",
      "from random import shuffle\n",
      "from collections import Counter\n",
      "from nltk.stem.wordnet import WordNetLemmatizer\n",
      "from nltk.tokenize.punkt import PunktWordTokenizer\n",
      "import nltk.tag, nltk.data\n",
      "from nltk.corpus import wordnet\n",
      "from nltk.stem.porter import PorterStemmer\n",
      "import numpy as np\n",
      "from operator import itemgetter\n",
      "import csv\n",
      "import os\n",
      "import collections\n",
      "import operator\n",
      "from collections import OrderedDict\n",
      "import string\n",
      "import re\n",
      "import mysql.connector\n",
      "from pandas import DataFrame\n",
      "import pandas.io.sql as sql"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 33
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def connect_db(dbuser, dbpasswd, dbhost, dbenviron):\n",
      "  config = {\n",
      "  'user': dbuser,\n",
      "  'password': dbpasswd,\n",
      "  'host': dbhost,\n",
      "   'database': 'archextract_' + dbenviron,\n",
      "  'raise_on_warnings': True,\n",
      "  }\n",
      "  cnx = mysql.connector.connect(**config)\n",
      "  return cnx"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 34
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class PreprocessText:\n",
      "\n",
      "    #function to remove punct.\n",
      "    def remove_punct(self, text):\n",
      "      exclude = set(string.punctuation)\n",
      "      table = string.maketrans(\"\",\"\")\n",
      "      text = text.translate(table, string.punctuation)\n",
      "      return text\n",
      "\n",
      "    #remove stopwords-> A quick way to reduce elminate words that aren't valid key words.\n",
      "    def removestopwords(self, tokens, ):\n",
      "      def get_extrawords:\n",
      "      stopwords = nltk.corpus.stopwords.words('english')\n",
      "      tokens = [w for w in tokens if w.lower().strip() not in stopwords]\n",
      "      return tokens\n",
      "\n",
      "    ##lemmatize the words to reduce dimensionality. Also,option to do lemmatization based on POS.\n",
      "    #wordnet lemmatizer assumes everything is a noun unless otherwise specified, so we need to give\n",
      "    #it the wordnet pos if we don't want the default noun lookup.\n",
      "    def lemmatize(self, tokens, lemmatize_pos):\n",
      "        def get_wordnet_pos( pos_tag):\n",
      "            if pos_tag[1].startswith('J'):\n",
      "                return (pos_tag[0], wordnet.ADJ)\n",
      "            elif pos_tag[1].startswith('V'):\n",
      "                return (pos_tag[0], wordnet.VERB)\n",
      "            elif pos_tag[1].startswith('N'):\n",
      "                return (pos_tag[0], wordnet.NOUN)\n",
      "            elif pos_tag[1].startswith('R'):\n",
      "                return (pos_tag[0], wordnet.ADV)\n",
      "            else:\n",
      "                return (pos_tag[0], wordnet.NOUN)\n",
      "        lemmatizer = WordNetLemmatizer()\n",
      "        if lemmatize_pos:\n",
      "            filtered_pos =  [(x[0], x[1]) for x in tokens_pos if x[1] in ('NN', 'NNS' 'JJ') and len(x[0]) > 1]\n",
      "            #print filtered_pos\n",
      "            tokens_pos_wordnet = [ get_wordnet_pos(token) for token in filtered_pos]\n",
      "            tokens_lemm = [lemmatizer.lemmatize(token[0], token[1]) for token in tokens_pos_wordnet]\n",
      "        else:\n",
      "            tokens_lemm = [lemmatizer.lemmatize(token) for token in tokens]\n",
      "        return tokens_lemm\n",
      "    \n",
      "    #function that combines above functions in one routine\n",
      "    #lots of args to specify what preprocessing routine you want to use\n",
      "    def preprocess_txt(self, text, convertlower=True, nopunk=True, stopwords=True, lemmatize_doc=True, lemmatize_pos=True, stemmed=False):\n",
      "      #convert to lower\n",
      "      if convertlower:\n",
      "        text = text.lower()\n",
      "      # remove punctuation\n",
      "      if nopunk:\n",
      "        text = self.remove_punct(text)\n",
      "      #tokenize text\n",
      "      tokens = PunktWordTokenizer().tokenize(text)\n",
      "      #remove extra whitespace$s\n",
      "      tokens = [token.strip() for token in tokens]\n",
      "      if stopwords:\n",
      "        tokens = self.removestopwords(tokens)\n",
      "      #lemmatize\n",
      "      if lemmatize_doc:\n",
      "        tokens = self.lemmatize(tokens,lemmatize_pos)\n",
      "      #stem\n",
      "      if stemmed:\n",
      "        porter = PorterStemmer()\n",
      "        tokens = [ porter.stem(token) for token in tokens ]\n",
      "      return tokens\n",
      "      #combine the tokens back into a string...need to do this for the tfidf vectorizer\n",
      "      #token_line = \" \".join(tokens)\n",
      "      #return token_line"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 45
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def connect_db(dbuser, dbpasswd, dbhost, dbenviron):\n",
      "  config = {\n",
      "  'user': dbuser,\n",
      "  'password': dbpasswd,\n",
      "  'host': dbhost,\n",
      "   'database': 'archextract_' + dbenviron,\n",
      "  'raise_on_warnings': True,\n",
      "  }\n",
      "  cnx = mysql.connector.connect(**config)\n",
      "  return cnx\n",
      "\n",
      "\n",
      "def db_variables():\n",
      "      dbhost  = '127.0.0.1'\n",
      "      dbpasswd= \"mypass\"\n",
      "      dbuser = \"myrailsbuddy\"\n",
      "      dbenviron = 'development'\n",
      "      return  dbuser, dbpasswd, dbhost, dbenviron\n",
      "    \n",
      "dbuser, dbpasswd, dbhost, dbenviron = db_variables()\n",
      "cnx = connect_db(dbuser, dbpasswd, dbhost, dbenviron)\n",
      "sqlq = \"select id, name from documents limit 2;\"\n",
      "df = sql.read_frame(sqlq, cnx)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>id</th>\n",
        "      <th>name</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td> 1</td>\n",
        "      <td> john_muir_papers_txt_kt1g5034t3.txt</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1</th>\n",
        "      <td> 2</td>\n",
        "      <td> john_muir_papers_txt_kt5s2035n6.txt</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "<p>2 rows \u00d7 2 columns</p>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 36,
       "text": [
        "   id                                 name\n",
        "0   1  john_muir_papers_txt_kt1g5034t3.txt\n",
        "1   2  john_muir_papers_txt_kt5s2035n6.txt\n",
        "\n",
        "[2 rows x 2 columns]"
       ]
      }
     ],
     "prompt_number": 36
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "basedir = \"/home/j9/Desktop/archextract/public/src_corpora/John_Muir/input/\"\n",
      "for doc in df['name']:\n",
      "  docfile = open(basedir+doc, 'r')\n",
      "  text = docfile.read()\n",
      "  "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "kt5s2035n6\n",
        "\n",
        "I can't visit one in a score of them and get anything done in the way of real work. \n",
        " One of my new books is just published and I'll send you a copy which surely is better than a brief chat. \n",
        " \n",
        " With sincere regard \n",
        " \n",
        " faithfully yours \n",
        " \n",
        " John Muir \n",
        " \n",
        " Mrs. M.C. Ashley \n",
        " State Hospital \n",
        " Middletown \n",
        " illegible \n",
        " \n",
        " letterhead \n",
        " \n",
        " New York \n",
        " \n",
        " June 12, 1911 - \n",
        " \n",
        " Dear Mrs Ashley: \n",
        " \n",
        " Your two letters with kind invitations to Middletown at last reached me. And nothing I assure you would be more delightful were time available. \n",
        " Never before have I been compelled to disappoint so many friends as now. \n",
        " \n",
        " John Muir \n",
        "\n"
       ]
      }
     ],
     "prompt_number": 46
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 31
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}